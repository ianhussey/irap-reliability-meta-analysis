---
title: "Process data for meta analyses of the IRAP's reliability"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)
```

```{r}

# set seed for computational reproducibility
set.seed(42)

# dependencies
library(tidyverse)
library(metafor)
library(knitr)
library(kableExtra)
library(psych)
library(readxl)
library(janitor)

# n iterations in permutation resampling procedures
n_iterations <- 1000

# function to round all numeric vars in a data frame
round_df <- function(df, n_digits = 3) {
  df %>% mutate_if(is.numeric, round, digits = n_digits)
}

spearman_brown_correction <- function(r_pearson) {
  inversion <- ifelse(r_pearson < 0, -1, 1)
  val <- round(2*abs(r_pearson)/(1 + abs(r_pearson)), 2)
  return(val*inversion)
}

apa_p_value <- function(p){
  p_formatted <- ifelse(p >= 0.001, paste("=", round(p, 3)),
                        ifelse(p < 0.001, "< .001", NA))
  p_formatted <- gsub(pattern = "0.", replacement = ".", x = p_formatted, fixed = TRUE)
  p_formatted
}

add_heterogeneity_metrics_to_forest <- function(fit) {
  bquote(paste("RE Model (", tau^2, " = ", .(formatC(round(fit$tau2, 1))), 
               ", ", I^2, " = ", .(formatC(round(fit$I2, 1))),
               "%, ", H^2," = ", .(formatC(round(fit$H2, 1))), ")"))
}

# using Ruscio's own code
ruscios_A_function <- function(x, y) {
  nx <- length(x)
  ny <- length(y)
  rx <- sum(rank(c(x, y))[1:nx])
  A = (rx / nx - (nx + 1) / 2) / ny
  return(A)
}

ruscios_A <- function(data, variable = "rt", group = "block_type", value1 = "incon", value2 = "con") {
  # Ensure data is a data frame (e.g., not a tbl_data)
  data <- as.data.frame(data)
  # Select the observations for group 1
  x <- data[data[[group]] == value1, variable]
  # Select the observations for group 2
  y <- data[data[[group]] == value2, variable]
  A.obs <- ruscios_A_function(x, y)
  return(as.numeric(A.obs))
}

# test retest icc
calculate_icc <- function(data) {
  require(psych)
  
  fit <- data %>%
    select(baseline, followup) %>%
    ICC(lmer = TRUE)
  
  results <- fit$results %>%
    filter(type == "ICC2") %>%
    select(type, ICC, ICC_ci_lower = "lower bound", ICC_ci_upper = "upper bound") %>%
    mutate(se = (ICC_ci_upper - ICC_ci_lower)/(1.96*2))
  
  return(results)
}

# table format in output
options(knitr.table.format = "html") 

# create directory needed to save output
dir.create("permutations")

```

# Get data and exclude outliers  

Exclude outliers on the basis of median mean_rt +/- 2MAD by domain

```{r}

# all
data_all <- read_csv("../trial level/data_trial_level.csv") 

# exclusions
data_outliers <- data_all %>%
  distinct(unique_id, .keep_all = TRUE) %>%
  select(unique_id, domain, mean_rt) %>%
  mutate(median_mean_rt = median(mean_rt, na.rm = TRUE),
         mad_mean_rt = mad(mean_rt, na.rm = TRUE)) %>%
  # exclude median +- 2MAD
  mutate(rt_outlier = ifelse(mean_rt < median_mean_rt-mad_mean_rt*2 |
                               mean_rt > median_mean_rt+mad_mean_rt*2, TRUE, FALSE)) %>%
  filter(rt_outlier == FALSE) %>%
  select(unique_id, rt_outlier) %>%
  full_join(data_all, by = "unique_id") %>%
  mutate(rt_outlier = ifelse(is.na(rt_outlier), TRUE, rt_outlier))

data_outliers_removed <- data_outliers %>%
  filter(rt_outlier == FALSE)

# subset ic
data_ic <- data_outliers_removed %>%
  filter(timepoint == "baseline")

# subset trt
data_unique_ids_two_timepoints <- data_outliers_removed %>%
  filter(timepoint %in% c("baseline", "followup")) %>%
  distinct(unique_id, timepoint, .keep_all = TRUE) %>%
  select(unique_id, timepoint, domain) %>%
  mutate(present = TRUE) %>%
  spread(timepoint, present) %>%
  filter(baseline == TRUE & followup == TRUE) %>%
  select(unique_id) 

data_trt <- data_unique_ids_two_timepoints %>%
  left_join(data_outliers, by = "unique_id")

```

# Demographics & sample sizes

Whole sample

```{r}

# write demographics to disk
data_demographics <- data_outliers %>%
  distinct(unique_id, .keep_all = TRUE) %>%
  select(unique_id, age, gender, rt_outlier)

write_csv(data_demographics, "data_demographics.csv")


# print sample sizes
data_outliers %>%
  distinct(unique_id, .keep_all = TRUE) %>%
  count(rt_outlier) %>%
  rename(n_outlier_exclusions = n) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_ic %>%
  distinct(unique_id, .keep_all = TRUE) %>%
  count(domain) %>%
  rename(n_ic_after_exlucions = n) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_ic %>%
  distinct(unique_id, .keep_all = TRUE) %>%
  count() %>%
  rename(n_ic_after_exlucions = n) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_trt %>%
  distinct(unique_id, .keep_all = TRUE) %>%
  count(domain) %>%
  rename(n_trt_after_exlucions = n) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_trt %>%
  distinct(unique_id, .keep_all = TRUE) %>%
  count() %>%
  rename(n_trt_after_exlucions = n) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

# Internal consistency

Three methods of calculating internal consistency for the Implicit Relational Assessment Procedure (IRAP): 
- Odd vs even trials (mostly commonly reported in literature)
- First vs second half of task (more common for the most popular implicit measure, the IAT; would allow a like-for-like comparison)
- Cronbach's alpha via permutation (most robust)

## Split-half reliability via odd vs. even trials

### Score

```{r}

data_D_scores_ic_oddeventrials <- data_ic %>%
  filter(rt <= 10000) %>%
  group_by(unique_id, trial_odd_even) %>%
  summarize(mean_con = mean(rt[block_type == "con"], na.rm = TRUE),
            mean_incon = mean(rt[block_type == "incon"], na.rm = TRUE),
            sd = mean(rt, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(D = (mean_incon - mean_con)/sd) %>%
  select(unique_id, D, trial_odd_even) %>%
  spread(trial_odd_even, D) %>%
  left_join(select(distinct(data_ic, unique_id, .keep_all = TRUE), unique_id, domain), by = "unique_id")

```

### Calculate effect sizes

Bonett transformations 

```{r}

# calculate internal consistencies
data_D_scores_internal_consistency_oddeventrials <- data_D_scores_ic_oddeventrials %>%
  dplyr::select(domain, odd, even) %>%
  na.omit() %>%
  group_by(domain) %>%
  summarize(r_pearson = cor(odd, even),
            n = n()) %>%
  ungroup() %>%
  mutate(r_sb = spearman_brown_correction(r_pearson),
         parts = 2) %>%
  escalc(measure = "ABT", 
         ai = r_sb, 
         mi = parts, 
         ni = n,
         data = .) 

write_csv(data_D_scores_internal_consistency_oddeventrials, "data_D_scores_internal_consistency_oddeventrials.csv")

```

## Split-half reliability via first vs. second half of the task

```{r}

data_n_trials_halves <- data_ic %>%
  group_by(unique_id) %>%
  summarize(total_n_trials = n()) 

data_halves <- data_ic %>%
  left_join(data_n_trials_halves, by = "unique_id") %>%
  group_by(unique_id) %>%
  mutate(trial_n = row_number(),
         half = ifelse(trial_n > (total_n_trials/2), "second", "first")) %>%
  ungroup()

```

### Score

```{r}

data_D_scores_ic_firstsecond <- data_halves %>%
  filter(rt <= 10000) %>%
  group_by(unique_id, half) %>%
  summarize(mean_con = mean(rt[block_type == "con"], na.rm = TRUE),
            mean_incon = mean(rt[block_type == "incon"], na.rm = TRUE),
            sd = mean(rt, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(D = (mean_incon - mean_con)/sd) %>%
  select(unique_id, D, half) %>%
  spread(half, D) %>%
  rename(first = first, 
         second = second) %>%
  left_join(select(distinct(data_ic, unique_id, .keep_all = TRUE), unique_id, domain), by = "unique_id")

```

### Calculate effect sizes

Bonett transformations 

```{r}

# calculate internal consistencies
data_D_scores_internal_consistency_firstsecondhalves <- data_D_scores_ic_firstsecond %>%
  dplyr::select(domain, first, second) %>%
  na.omit() %>%
  group_by(domain) %>%
  summarize(r_pearson = cor(first, second),
            n = n()) %>%
  ungroup() %>%
  mutate(r_sb = spearman_brown_correction(r_pearson),
         parts = 2) %>%
  escalc(measure = "ABT", 
         ai = r_sb, 
         mi = parts, 
         ni = n,
         data = .) 

write_csv(data_D_scores_internal_consistency_firstsecondhalves,
          "data_D_scores_internal_consistency_firstsecondhalves.csv")

```

## Permutation-based split half correlations 

ie an estimate of alpha through large number number of random half splits (see Parsons, Kruijt, & Fox. 2019).

Because of the large number of D/A scores that are calculated (i.e., 2 per participant \* N participants \* N permutations = several million scores) this section can take a several minutes to run. I've already done some timing optimization to speed up this process by a factor of 5, but perhaps more is possible.

### Score

```{r}

# slow runtime - circa 10 minutes, so check if there's saved results already first
if (file.exists("permutations/data_D_scores_internal_consistency_permutations.RData")) {
  
  load("permutations/data_D_scores_internal_consistency_permutations.RData")
  
} else {
  
  # find domain for each id - to be joined into df further below
  data_domain_for_each_id <- data_ic %>%
    distinct(unique_id, .keep_all = TRUE) %>%
    select(unique_id, domain)
  
  # create a temp list
  data_internal_consistency_list = list()
  
  # define progress bar
  step_i <- 0
  pb = txtProgressBar(min = 0, max = n_iterations, initial = 0)
  
  # assign row ids
  data_ic_rownumbers <- data_ic %>%
    arrange(unique_id, block_type) %>%
    rownames_to_column(var = "row_id") %>%
    group_by(unique_id, block_type) %>%
    mutate(trial_number = row_number()) %>%
    ungroup()
  
  # apply workflow and append results to list
  for(i in 1:n_iterations){
    
    # sample exactly half of the data and label as subset 'a' vs 'b'
    max_trials_per_block <- data_ic_rownumbers %>%
      summarize(max_trials_per_block = max(trial_number)) %>%
      pull(max_trials_per_block)
    
    trial_subset_a <- data.frame(trial_number = seq(from = 1, to = max_trials_per_block, by = 1)) %>% 
      sample_frac(size = 0.5) %>%
      arrange(trial_number) %>%
      pull(trial_number)
    
    data_ic_rownumbers_with_subsets <- data_ic_rownumbers %>%
      mutate(subset = ifelse(trial_number %in% trial_subset_a, "a", "b")) %>%
      arrange(unique_id, block_type, trial_number)
    
    # # calculate a metric of how ordered this randomized subset of the trials are
    # ## start by creating the 'b' vector
    # temp <- seq(from = 1, to = max_trials_per_block, by = 1)
    # trial_subset_b <- temp[!temp %in% trial_subset_a]
    # 
    # ## probability that b > a
    # probability_of_superiority <- ruscios_A_function(trial_subset_b, trial_subset_a)
    # 
    # subset_orderedness <- abs(probability_of_superiority - 0.50) + 0.5
    
    # then calculate D1 scores (Greenwald, Banaji & Nosek, 2003)
    # these involve: 1) trimming RTs > 10000 ms, 2) finding a mean rt for each block type, 3) findings an sd for all the pooled RTs across blocks, 4) D = (mean_incon - mean_con)/sd
    data_temp_1 <- data_ic_rownumbers_with_subsets %>%
      filter(rt <= 10000)
    
    data_temp_2 <- data_temp_1 %>%
      group_by(unique_id, subset, block_type) %>%
      summarize(mean = mean(rt, na.rm = TRUE)) %>%
      ungroup() %>%
      spread(block_type, mean) %>%
      rename(mean_con = con,
             mean_incon = incon)
    
    data_temp_3 <- data_temp_1 %>%
      group_by(unique_id, subset) %>%
      summarize(sd = mean(rt, na.rm = TRUE)) %>%
      ungroup()
    
    data_internal_consistency <- left_join(data_temp_2, data_temp_3, by = c("unique_id", "subset")) %>%
      # finish calculating D scores for each participant and subset
      mutate(D = (mean_incon - mean_con)/sd) %>%
      select(unique_id, D, subset) %>%
      spread(subset, D) %>%
      rename(D_a = a,
             D_b = b) %>%
      # re add the study domain
      left_join(data_domain_for_each_id, by = "unique_id") %>%
      # calculate correlations between subsets
      na.omit() %>%
      group_by(domain) %>%
      summarize(r_pearson = cor(D_a, D_b)) %>%
      ungroup() %>%
      # apply spearman brown correction
      mutate(r_sb = spearman_brown_correction(r_pearson)) %>%
      # add iteration
      mutate(iteration = i)
    #orderedness = subset_orderedness)
    
    data_internal_consistency_list[[i]] <- data_internal_consistency
    
    # update progress bar
    step_i = step_i + 1
    setTxtProgressBar(pb, step_i)
    
  }
  
  # flatten list and parse results
  data_D_scores_internal_consistency_permutations <- dplyr::bind_rows(data_internal_consistency_list)
  
  # save
  save(data_D_scores_internal_consistency_permutations, 
       file = "permutations/data_D_scores_internal_consistency_permutations.RData")
  
}

```

### Calculate effect sizes

Estimate of alpha obtained via mean of the resampled Spearman-Brown corrected split half correlations, when calculating D1 scores for each half and sampling 2000 permutations.   

Bonett transformations 

NB forest plot above includes empirical CIs across the permutation resamples using the percentile method, whereas the forest plot below estimates CIs based on the variance calculated from the Fischer's r-to-z and N. 

Also includes response option location for later moderator meta analysis

```{r fig.height=8, fig.width=8}

data_n_per_domain <- data_ic %>%
  distinct(unique_id, .keep_all = TRUE) %>%
  count(domain) %>%
  select(domain, n)

data_response_option_locations <- data_ic %>%
  distinct(domain, 
           response_option_locations, 
           trials_per_block, 
           number_of_pairs_of_test_blocks, 
           response_option_locations)

data_D_scores_internal_consistency_permuted_estimates <- data_D_scores_internal_consistency_permutations %>%
  group_by(domain) %>%
  dplyr::summarize(alpha = mean(r_sb, na.rm = TRUE),
                   alpha_ci_lower = quantile(r_sb, 0.025),
                   alpha_ci_upper = quantile(r_sb, 0.975)) %>%
  left_join(data_n_per_domain, by = "domain") %>%
  left_join(data_response_option_locations, by = "domain") %>%
  mutate(parts = 2) %>%
  escalc(measure = "ABT", 
         ai = alpha, 
         mi = parts, 
         ni = n,
         data = .)

write_csv(data_D_scores_internal_consistency_permuted_estimates,
          "data_D_scores_internal_consistency_permuted_estimates.csv")

```

# Test-retest reliability

## Score

```{r}

data_D_scores_trt <- data_trt %>%
  filter(rt <= 10000) %>%
  group_by(unique_id, timepoint) %>%
  summarize(mean_con = mean(rt[block_type == "con"], na.rm = TRUE),
            mean_incon = mean(rt[block_type == "incon"], na.rm = TRUE),
            sd = mean(rt, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(D = (mean_incon - mean_con)/sd) %>%
  select(unique_id, D, timepoint) %>%
  spread(timepoint, D) %>%
  left_join(select(distinct(data_trt, unique_id, .keep_all = TRUE), unique_id, domain), by = "unique_id")

```

```{r fig.height=7, fig.width=7}

ggplot(data_D_scores_trt, aes(baseline, followup)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~domain)

```

## Calculate effect sizes

### Pearson's r

```{r}

data_D_scores_test_retest_r <- data_D_scores_trt %>%
  group_by(domain) %>%
  summarize(r = cor(baseline, followup, method = "pearson", use = "pairwise.complete.obs")) %>%
  ungroup() %>%
  left_join(data_D_scores_trt %>%
              count(domain), by = "domain") %>%
  escalc(measure = "ZCOR", 
         ri = r, 
         ni = n,
         data = .)

write_csv(data_D_scores_test_retest_r, "data_D_scores_test_retest_r.csv")

```

### ICC

```{r}

data_D_scores_test_retest_icc <- data_D_scores_trt %>%
  group_by(domain) %>%
  do(calculate_icc(.)) %>%
  ungroup() %>%
  left_join(data_D_scores_trt %>%
              count(domain), by = "domain")

write_csv(data_D_scores_test_retest_icc, "data_D_scores_test_retest_icc.csv")

```

Variance associated with each effect size calculated mathematically using the effect size and sample size.

# Attempts to improve internal consistency

For moderation meta-analysis. 

Internal consistency only as test-retest doesn't have sufficient k studies to investigate these questions with any power.

D scores and permutation split half only, as interactive effects of changing multiple things is beyond the scope of our article.

## Alternative scoring method: A scores

### Score

```{r}

if (file.exists("permutations/data_A_scores_internal_consistency_permutations.RData")) {
  
  load("permutations/data_A_scores_internal_consistency_permutations.RData")
  
} else {
  
  # find domain for each id - to be joined into df further below
  data_domain_for_each_id <- data_ic %>%
    distinct(unique_id, .keep_all = TRUE) %>%
    select(unique_id, domain)
  
  # create a temp list
  data_internal_consistency_list = list()
  
  # define progress bar
  step_i <- 0
  pb = txtProgressBar(min = 0, max = n_iterations, initial = 0)
  
  # assign row ids
  data_ic_rownumbers <- data_ic %>%
    arrange(unique_id, block_type) %>%
    rownames_to_column(var = "row_id") %>%
    group_by(unique_id, block_type) %>%
    mutate(trial_number = row_number()) %>%
    ungroup()
  
  # apply workflow and append results to list
  for(i in 1:n_iterations){
    
    # sample exactly half of the data and label as subset 'a' vs 'b'
    max_trials_per_block <- data_ic_rownumbers %>%
      summarize(max_trials_per_block = max(trial_number)) %>%
      pull(max_trials_per_block)
    
    trial_subset_a <- data.frame(trial_number = seq(from = 1, to = max_trials_per_block, by = 1)) %>% 
      sample_frac(size = 0.5) %>%
      arrange(trial_number) %>%
      pull(trial_number)
    
    # then calculate A scores
    data_internal_consistency <- data_ic_rownumbers %>%
      mutate(subset = ifelse(trial_number %in% trial_subset_a, "a", "b")) %>%
      group_by(unique_id, subset) %>%
      # calculate A scores for each participant and subset
      do(A = ruscios_A(data = .)) %>%
      ungroup() %>%
      mutate(A = as.numeric(A)) %>%
      select(unique_id, A, subset) %>%
      spread(subset, A) %>%
      rename(A_a = a,
             A_b = b) %>%
      
      # re add the study domain
      left_join(data_domain_for_each_id, by = "unique_id") %>%
      # calculate correlations between subsets
      na.omit() %>%
      group_by(domain) %>%
      summarize(r_pearson = cor(A_a, A_b)) %>%
      ungroup() %>%
      # apply spearman brown correction
      mutate(r_sb = spearman_brown_correction(r_pearson)) %>%
      # add iteration
      mutate(iteration = i)
    #orderedness = subset_orderedness)
    
    data_internal_consistency_list[[i]] <- data_internal_consistency
    
    # update progress bar
    step_i = step_i + 1
    setTxtProgressBar(pb, step_i)
    
  }
  
  # flatten list and parse results
  data_A_scores_internal_consistency_permutations <- dplyr::bind_rows(data_internal_consistency_list)
  
  # save
  save(data_A_scores_internal_consistency_permutations, 
       file = "permutations/data_A_scores_internal_consistency_permutations.RData")
  
}

```

### Calculate effect sizes

Estimate of alpha obtained via mean of the resampled Spearman-Brown corrected split half correlations, when calculating D1 scores for each half and sampling 2000 permutations.   

Bonett transformations 

NB forest plot above includes empirical CIs across the permutation resamples using the percentile method, whereas the forest plot below estimates CIs based on the variance calculated from the Fischer's r-to-z and N. 

```{r fig.height=8, fig.width=8}

data_n_per_domain <- data_ic %>%
  distinct(unique_id, .keep_all = TRUE) %>%
  count(domain) %>%
  select(domain, n)

data_A_scores_internal_consistency_permuted_estimates <- data_A_scores_internal_consistency_permutations %>%
  group_by(domain) %>%
  dplyr::summarize(alpha = mean(r_sb, na.rm = TRUE),
                   alpha_ci_lower = quantile(r_sb, 0.025),
                   alpha_ci_upper = quantile(r_sb, 0.975)) %>%
  left_join(data_n_per_domain, by = "domain") %>%
  mutate(parts = 2) %>%
  escalc(measure = "ABT", 
         ai = alpha, 
         mi = parts, 
         ni = n,
         data = .)

write_csv(data_A_scores_internal_consistency_permuted_estimates,
          "data_A_scores_internal_consistency_permuted_estimates.csv")

```

## Difrerences in block order

### Score

```{r}

# slow runtime - circa 10 minutes, so check if there's saved results already first
if (file.exists("permutations/data_D_scores_internal_consistency_permutations_by_block_order.RData")) {
  
  load("permutations/data_D_scores_internal_consistency_permutations_by_block_order.RData")
  
} else {
  
  # filter domains that have both block_types
  data_ic_subset <- data_ic %>%
    filter(domain %in% (data_ic %>%
                          distinct(domain, block_order) %>%
                          count(domain) %>%
                          filter(n == 2) %>%
                          pull(domain)))
  
  # find domain for each id - to be joined into df further below
  data_domain_for_each_id <- data_ic_subset %>%
    distinct(unique_id, .keep_all = TRUE) %>%
    select(unique_id, domain, block_order)
  
  # create a temp list
  data_internal_consistency_list = list()
  
  # define progress bar
  step_i <- 0
  pb = txtProgressBar(min = 0, max = n_iterations, initial = 0)
  
  # assign row ids
  data_ic_rownumbers <- data_ic_subset %>%
    arrange(unique_id, block_type) %>%
    rownames_to_column(var = "row_id") %>%
    group_by(unique_id, block_type) %>%
    mutate(trial_number = row_number()) %>%
    ungroup()
  
  # apply workflow and append results to list
  for(i in 1:n_iterations){
    
    # sample exactly half of the data and label as subset 'a' vs 'b'
    max_trials_per_block <- data_ic_rownumbers %>%
      summarize(max_trials_per_block = max(trial_number)) %>%
      pull(max_trials_per_block)
    
    trial_subset_a <- data.frame(trial_number = seq(from = 1, to = max_trials_per_block, by = 1)) %>% 
      sample_frac(size = 0.5) %>%
      arrange(trial_number) %>%
      pull(trial_number)
    
    data_ic_rownumbers_with_subsets <- data_ic_rownumbers %>%
      mutate(subset = ifelse(trial_number %in% trial_subset_a, "a", "b")) %>%
      arrange(unique_id, block_type, trial_number)
    
    data_temp_1 <- data_ic_rownumbers_with_subsets %>%
      filter(rt <= 10000)
    
    data_temp_2 <- data_temp_1 %>%
      group_by(unique_id, subset, block_type) %>%
      summarize(mean = mean(rt, na.rm = TRUE)) %>%
      ungroup() %>%
      spread(block_type, mean) %>%
      rename(mean_con = con,
             mean_incon = incon)
    
    data_temp_3 <- data_temp_1 %>%
      group_by(unique_id, subset) %>%
      summarize(sd = mean(rt, na.rm = TRUE)) %>%
      ungroup()
    
    data_internal_consistency <- 
      left_join(data_temp_2, data_temp_3, by = c("unique_id", "subset")) %>%
      # finish calculating D scores for each participant and subset
      mutate(D = (mean_incon - mean_con)/sd) %>%
      select(unique_id, D, subset) %>%
      spread(subset, D) %>%
      rename(D_a = a,
             D_b = b) %>%
      # re add the study domain
      left_join(data_domain_for_each_id, by = "unique_id") %>%
      # calculate correlations between subsets
      na.omit() %>%
      group_by(domain, block_order) %>%
      summarize(r_pearson = cor(D_a, D_b)) %>%
      ungroup() %>%
      # apply spearman brown correction
      mutate(r_sb = spearman_brown_correction(r_pearson)) %>%
      # add iteration
      mutate(iteration = i)
    #orderedness = subset_orderedness)
    
    data_internal_consistency_list[[i]] <- data_internal_consistency
    
    # update progress bar
    step_i = step_i + 1
    setTxtProgressBar(pb, step_i)
    
  }
  
  # flatten list and parse results
  data_D_scores_internal_consistency_permutations_by_block_order <- dplyr::bind_rows(data_internal_consistency_list)
  
  # save
  save(data_D_scores_internal_consistency_permutations_by_block_order, 
       file = "permutations/data_D_scores_internal_consistency_permutations_by_block_order.RData")
  
}

```

### Calculate effect sizes

```{r fig.height=8, fig.width=8}

data_n_per_domain_subset <- data_ic %>%
  distinct(unique_id, .keep_all = TRUE) %>%
  count(domain, block_order) %>%
  select(domain, block_order, n)

data_D_scores_internal_consistency_permuted_estimates_by_block_order <- 
  data_D_scores_internal_consistency_permutations_by_block_order %>%
  group_by(domain, block_order) %>%
  dplyr::summarize(alpha = mean(r_sb, na.rm = TRUE),
                   alpha_ci_lower = quantile(r_sb, 0.025),
                   alpha_ci_upper = quantile(r_sb, 0.975)) %>%
  ungroup() %>%
  left_join(data_n_per_domain_subset, by = c("domain", "block_order")) %>%
  mutate(parts = 2) %>%
  escalc(measure = "ABT", 
         ai = alpha, 
         mi = parts, 
         ni = n,
         data = .)

write_csv(data_D_scores_internal_consistency_permuted_estimates_by_block_order,
          "data_D_scores_internal_consistency_permuted_estimates_by_block_order.csv")

```

## Differences in response option positions 

This data is already integrated into the permutation based estimate of internal consistency and requires no more processing; that dataset can be used in the moderator meta-analysis.

# Session info

```{r}

sessionInfo()

```

