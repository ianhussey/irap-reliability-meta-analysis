---
title: "Meta analyses of the IRAP's reliability"
subtitle: "Scores for each individual trial type"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)
```

```{r}

set.seed(42)

# dependencies
library(tidyverse)
library(metafor)
library(knitr)
library(kableExtra)
library(psych)
library(ggExtra)
library(scales) 
library(janitor)
library(parallel)
#library(ggtext) # devtools::install_github("clauswilke/ggtext")


# function to round all numeric vars in a data frame
round_df <- function(df, n_digits = 3) {
  df %>% mutate_if(is.numeric, janitor::round_half_up, digits = n_digits)
}

probability_of_superiority_function <- function(x, y) {
  nx <- length(x)
  ny <- length(y)
  rx <- sum(rank(c(x, y))[1:nx])
  A = (rx / nx - (nx + 1) / 2) / ny
  return(A)
}

spearman_brown_correction <- function(r_pearson) {
  inversion <- ifelse(r_pearson < 0, -1, 1)
  val <- janitor::round_half_up(2*abs(r_pearson)/(1 + abs(r_pearson)), 2)
  return(val*inversion)
}

apa_p_value <- function(p){
  p_formatted <- ifelse(p >= 0.001, paste("=", janitor::round_half_up(p, 3)),
                        ifelse(p < 0.001, "< .001", NA))
  p_formatted <- gsub(pattern = "0.", replacement = ".", x = p_formatted, fixed = TRUE)
  p_formatted
}

add_heterogeneity_metrics_to_forest <- function(fit) {
  bquote(paste("RE Model (", tau^2, " = ", .(formatC(janitor::round_half_up(fit$tau2, 2))), 
               ", ", I^2, " = ", .(formatC(janitor::round_half_up(fit$I2, 1))),
               "%, ", H^2," = ", .(formatC(janitor::round_half_up(fit$H2, 2))), ")"))
}

# table format in output
options(knitr.table.format = "html") 

# create directory needed to save output
dir.create("models")
dir.create("plots")

```

# Data 

```{r}

data_D_scores_internal_consistency_permuted_estimates <-
  read_csv("../data/effect sizes for meta-analyses/data_D_scores_internal_consistency_permuted_estimates_trial_types.csv")

data_D_scores_test_retest_icc <- 
  read_csv("../data/effect sizes for meta-analyses/data_D_scores_test_retest_icc_trial_types.csv")

data_D_scores <- read_csv("../data/scored/data_scored.csv") |>
  filter(met_performance_criteria_typical == TRUE &
           timepoint == 1) |>
  select(unique_id, domain, D_overall, D_tt1, D_tt2, D_tt3, D_tt4)

```

# Internal consistency

Three methods of calculating internal consistency for the Implicit Relational Assessment Procedure (IRAP): 
- Odd vs even trials (mostly commonly reported in literature)
- First vs second half of task (more common for the most popular implicit measure, the IAT; would allow a like-for-like comparison)
- Cronbach's alpha via permutation (most robust)

The IRAP effect is most often quantified using the D1 scoring algorithm (Greenwald, Banaji & Nosek, 2003). i.e., trim all RTs > 10,000 ms, then D1 = (mean RT in the inconsistent blocks - mean RT in the consistent blocks)/SD of RTs in both block types. D1 scores are used throughout. Because of this, it wasn't possible to use Sam Parson's splithalf R package. A workflow for permutation-based alpha estimation using D scores is provided below.  

In each case, correlations between D scores calculated from the split halves are calculated, and then the Spearman-Brown prophecy formula is applied, i.e., r_sb = 2*r_pearson / (1+r_pearson). Importantly, this correction is applied to the absolute value of the pearson's correlation, and then its sign is corrected to be congruent with the pearson's correlation. This ensures that r_sb has a possible range of -1 to +1, as correlations should (whereas correction of native negative correlations allow for a lower limit of -Inf). I haven't seen this treatment of SB corrections applied to negative correlations well explicated elsewhere, so it is important to note here. 

For meta analysis, spearman brown correlations are converted to Fischers r-to-z for meta analysis. Estimates of the meta effect are then converted back for reporting. Heterogeneity metrics represent heterogeneity in Fischer's r-to-z estimates. See [here](https://stats.stackexchange.com/questions/26485/does-transformation-of-r-into-fisher-z-benefit-a-meta-analysis).

## Permutation-based split half correlations

ie an estimate of alpha through large number number of random half splits (see Parsons, Kruijt, & Fox. 2019). Estimate of alpha obtained via mean of the resampled Spearman-Brown corrected split half correlations, when calculating D1 scores for each half and sampling 10000 permutations.

Plot bonett transformed and back-transformed estimates.

```{r fig.height=12, fig.width=6}

ggplot(data_D_scores_internal_consistency_permuted_estimates, 
       aes(y = transf.iabt(alpha), x = fct_rev(domain), group = trial_type)) + # , color = fct_rev(domain)
  geom_linerange(aes(ymin = transf.iabt(alpha_ci_lower), ymax = transf.iabt(alpha_ci_upper)),
                 position = position_dodge(width = 0.75)) +
  geom_point(position = position_dodge(width = 0.75)) +
  coord_flip() +
  xlab("Domain") +
  ylab("Alpha") +
  theme_linedraw() +
  scale_color_viridis_d() #begin = 0.3, end = 0.7

```

```{r fig.height=6, fig.width=8}

data_ic_permuted <- data_D_scores_internal_consistency_permuted_estimates %>%
  mutate(se = (alpha_ci_upper - alpha_ci_lower)/(1.96*2))

# fit random Effects model 
fit_internal_consistency_permuted_estimates <- 
  rma.mv(yi     = yi, 
         V      = vi, 
         random = ~ 1 | domain,
         data   = data_ic_permuted,
         slab   = domain)

# make predictions 
predictions_permutations <-
  predict(fit_internal_consistency_permuted_estimates, digits = 5) %>%
  as.data.frame() %>%
  round_df(2)

# plot
metafor::forest(data_ic_permuted$yi, 
                data_ic_permuted$vi,
                xlim = c(-0.5, 1.5), # adjust horizontal plot region limits
                transf = transf.iabt,
                xlab = bquote(paste("Cronbach's ", alpha)),
                #at = c(0, 0.2, 0.4, 0.6, 0.8, 1),
                at = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0),
                order = "obs", # order by size of yi
                slab = NA, 
                annotate = FALSE, # remove study labels and annotations
                efac = 0, # remove vertical bars at end of CIs
                pch = 19, # changing point symbol to filled circle
                col = "gray40", # change color of points/CIs
                psize = 2, # increase point size
                cex.lab = 1, cex.axis = 1, # increase size of x-axis title/labels
                lty = c("solid", "blank")) # remove horizontal line at top of plot
points(sort(transf.iabt(data_ic_permuted$yi)), 
       nrow(data_ic_permuted):1, pch = 19, cex = 0.5) # draw points one more time to make them easier to see
addpoly(fit_internal_consistency_permuted_estimates, mlab = "", cex = 1, addcred = TRUE) # add summary polygon at bottom and text
#text(0, -1, "RE Model", pos = 4, offset = 0, cex = 1)

# summarize results
meta_effect_ic <- 
  paste0("Meta analysis: k = ", fit_internal_consistency_permuted_estimates$k, 
         ", alpha = ",  janitor::round_half_up(transf.iabt(predictions_permutations$pred), 2), 
         ", 95% CI [", janitor::round_half_up(transf.iabt(predictions_permutations$ci.lb), 2), ", ", 
         janitor::round_half_up(transf.iabt(predictions_permutations$ci.ub), 2), "]", 
         ", 95% CR [", janitor::round_half_up(transf.iabt(predictions_permutations$pi.lb), 2), ", ", 
         janitor::round_half_up(transf.iabt(predictions_permutations$pi.ub), 2), "]") 

# save to disk for making pdf plot
#write_rds(fit_internal_consistency_permuted_estimates, "models/fit_internal_consistency_permuted_estimates.rds")

```

```{r eval=FALSE, fig.height=18, fig.width=7, include=FALSE}

metafor::forest(data_ic_permuted$yi, 
                data_ic_permuted$vi,
                slab = paste(data_ic_permuted$domain, data_ic_permuted$trial_type), 
                transf = transf.iabt,
                order = "obs")

```

Meta effect: `r meta_effect_ic`.

### Sensitivity analyses

```{r fig.height=6, fig.width=8}

data_ic_permuted_sensitivity <- data_D_scores_internal_consistency_permuted_estimates %>%
  filter(!str_detect(domain, "Sexuality")) %>%
  filter(!domain %in% c("Gender stereotypes (3)")) %>%
  mutate(se = (alpha_ci_upper - alpha_ci_lower)/(1.96*2))

# fit random Effects model 
fit_internal_consistency_permuted_estimates_sensitivity <- 
  rma.mv(yi     = yi, 
         V      = vi, 
         random = ~ 1 | domain,
         data   = data_ic_permuted_sensitivity,
         slab   = domain)

fit_internal_consistency_permuted_estimates_sensitivity

# make predictions 
predictions_permutations_sensitivity <-
  predict(fit_internal_consistency_permuted_estimates_sensitivity, digits = 5) %>%
  as.data.frame() %>%
  round_df(2)

# plot
metafor::forest(data_ic_permuted_sensitivity$yi, 
                data_ic_permuted_sensitivity$vi,
                xlim = c(-0.5, 1.5), # adjust horizontal plot region limits
                transf = transf.iabt,
                xlab = bquote(paste("Cronbach's ", alpha)),
                #at = c(0, 0.2, 0.4, 0.6, 0.8, 1),
                at = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0),
                order = "obs", # order by size of yi
                slab = NA, 
                annotate = FALSE, # remove study labels and annotations
                efac = 0, # remove vertical bars at end of CIs
                pch = 19, # changing point symbol to filled circle
                col = "gray40", # change color of points/CIs
                psize = 2, # increase point size
                cex.lab = 1, cex.axis = 1, # increase size of x-axis title/labels
                lty = c("solid", "blank")) # remove horizontal line at top of plot
points(sort(transf.iabt(data_ic_permuted_sensitivity$yi)), 
       nrow(data_ic_permuted_sensitivity):1, pch = 19, cex = 0.5) # draw points one more time to make them easier to see
addpoly(fit_internal_consistency_permuted_estimates_sensitivity, mlab = "", cex = 1, addcred = TRUE) # add summary polygon at bottom and text
#text(0, -1, "RE Model", pos = 4, offset = 0, cex = 1)

# summarize results
meta_effect_sensitivity <- 
  paste0("Meta analysis: k = ", fit_internal_consistency_permuted_estimates_sensitivity$k, 
         ", alpha = ",  janitor::round_half_up(transf.iabt(predictions_permutations_sensitivity$pred), 2), 
         ", 95% CI [", janitor::round_half_up(transf.iabt(predictions_permutations_sensitivity$ci.lb), 2), ", ", 
         janitor::round_half_up(transf.iabt(predictions_permutations_sensitivity$ci.ub), 2), "]", 
         ", 95% CR [", janitor::round_half_up(transf.iabt(predictions_permutations_sensitivity$pi.lb), 2), ", ", 
         janitor::round_half_up(transf.iabt(predictions_permutations_sensitivity$pi.ub), 2), "]") 

#save to disk for making pdf plot
write_rds(fit_internal_consistency_permuted_estimates_sensitivity, "models/fit_internal_consistency_permuted_estimates_trial_types_sensitivity.rds")

```

```{r eval=FALSE, fig.height=18, fig.width=7, include=FALSE}

metafor::forest(data_ic_permuted_sensitivity$yi, 
                data_ic_permuted_sensitivity$vi,
                slab = paste(data_ic_permuted_sensitivity$domain, data_ic_permuted_sensitivity$trial_type), 
                transf = transf.iabt,
                order = "obs")

```

Meta effect: `r meta_effect_sensitivity`.

# Test-retest reliability

## Intraclass Correlations

"It has been argued that test-retest reliability should reflect absolute agreement, rather than consistency, between measurements (Koo & Li, 2016). For example, a perfect correlation between scores at two time points may occur also when there is a systematic difference between time points (i.e. a difference that is about equal for all participants)." (Parsons, Kruijt, & Fox, 2019).
Absolute agreement therefore also takes within-participant changes into account in its denominator, where consistency does not.

```{r fig.height=8.5, fig.width=8}

# fit random Effects model 
fit_test_retest_icc <- 
  rma.mv(yi     = ICC, 
         V      = se^2, 
         random = ~ 1 | domain,
         data   = data_D_scores_test_retest_icc,
         slab   = domain)

fit_test_retest_icc

# make predictions 
predictions_test_retest_icc <-
  predict(fit_test_retest_icc, digits = 5) %>%
  as.data.frame() %>%
  round_df(2) 

# plot
metafor::forest(fit_test_retest_icc,
                xlab = "Intraclass Correlation",
                addcred = TRUE,
                refline = FALSE,
                xlim = c(-3, 2.3),
                #at = c(-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1),
                at = c(-1, -0.5, 0, 0.5, 1))
                #mlab = add_heterogeneity_metrics_to_forest(fit_test_retest_icc))
text(-3, 34, "Study", pos = 4)
text(2.3, 34, "ICC2 [95% CI]", pos = 2)

# summarize results
meta_effect_trt <- 
  paste0("Meta analysis: k = ", fit_test_retest_icc$k, 
         ", ICC2 = ",  janitor::round_half_up(predictions_test_retest_icc$pred, 2), 
         ", 95% CI [", janitor::round_half_up(predictions_test_retest_icc$ci.lb, 2), ", ", 
         janitor::round_half_up(predictions_test_retest_icc$ci.ub, 2), "]", 
         ", 95% CR [", janitor::round_half_up(predictions_test_retest_icc$pi.lb, 2), ", ", 
         janitor::round_half_up(predictions_test_retest_icc$pi.ub, 2), "]") 

# write to disk for pdf plots
#write_rds(fit_test_retest_icc, "models/fit_test_retest_icc.rds")

```

Meta effect: `r meta_effect_trt`.

# Individual level utility via Standard Error of Measurement

## Calculate weighted average SD across domains

```{r}

# n by domain
ns <- data_D_scores |>
  count(domain) 

# calculate weighted average (by N) of the mean and SD D score for each trial-type and domain, averaged across domains
average_mean_and_sd_trialtype <- data_D_scores |>
  select(-D_overall) |>
  gather(trial_type, D, c(D_tt1, D_tt2, D_tt3, D_tt4)) |>
  group_by(domain, trial_type) |>
  summarize(mean = mean(D),
            sd = sd(D),
            .groups = "drop") |>
  left_join(ns, by = "domain") |>
  summarize(weighted_mean_mean = weighted.mean(mean, w = n),
            weighted_mean_sd = weighted.mean(sd, w = n),
            .groups = "drop") |> 
  round_df(2)

average_mean_and_sd_trialtype

percentiles <- data_D_scores |>
  select(-D_overall) |>
  gather(trial_type, D, c(D_tt1, D_tt2, D_tt3, D_tt4)) |>
  summarize(D_percentile_0.025 = quantile(D, probs = 0.025),
            D_percentile_0.975 = quantile(D, probs = 0.975)) |> 
  round_df(2)

percentiles

```

## 95% CI on individuals' scores from SEM

```{r}

sem <- function(sd, r_trt){
  sem = sd * sqrt(1 - r_trt)
  return(sem)
}

sem_estimate <- sem(sd = average_mean_and_sd_trialtype |> pull(weighted_mean_sd), 
                    r_trt = janitor::round_half_up(predictions_test_retest_icc$pred, 2))

individual_95ci_half_width <- janitor::round_half_up(sem_estimate * 1.96, 2)

```

The precision of an individual's score is their (trial type level) *D* score ± `r individual_95ci_half_width`.

The weighted mean *D* score across all domains and trial types is `r average_mean_and_sd_trialtype |> pull(weighted_mean_mean)`, i.e., a slightly positive effect (slightly faster on the consistent blocks than the inconsistent blocks). 

However, due to the task's poor reliability, an individual who demonstrates a (trial type) *D* score = `r average_mean_and_sd_trialtype |> pull(weighted_mean_mean)` can actually only be said to have a score within the interval *D* = 95% [`r average_mean_and_sd_trialtype |> pull(weighted_mean_mean) - individual_95ci_half_width`, `r average_mean_and_sd_trialtype |> pull(weighted_mean_mean) + individual_95ci_half_width`], i.e. anywhere between *very* much faster on the consistent blocks and *very* much faster on the inconsistent blocks. 

Combine this with the fact that 95% of all observed trial-type level *D* scores in the dataset lie within the range = `r as.numeric(percentiles$D_percentile_0.025)` to `r as.numeric(percentiles$D_percentile_0.975)`. This means that estimation of the average participant's score covers more than the range (`r janitor::round_half_up(individual_95ci_half_width*2 / (as.numeric(percentiles$D_percentile_0.975) - as.numeric(percentiles$D_percentile_0.025)) * 100, 2)`%) of *D* scores observed across all participants. i.e., the average individual participant cannot be distinguished from the *D* scores demonstrated by **all** other participants, from extremely positive to extremely negative.

This uncertainty around individual's scores strongly limits the IRAP's utility to make inferences about individuals rather than groups. 

Similar arguments have been made using other individual level estimation methods (i.e., using bootstrapping approaches, see [Hussey 2020 'The Implicit Relational Assessment Procedure is not suitable for individual use'](https://psyarxiv.com/w2ygr/)). 

# Session info

```{r}

sessionInfo()

```

