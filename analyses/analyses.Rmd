---
title: "Meta analyses of the IRAP's reliability"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)
```

```{r}

# dependencies
library(tidyverse)
library(metafor)
library(knitr)
library(kableExtra)
library(psych)

# function to round all numeric vars in a data frame
round_df <- function(df, n_digits = 3) {
  df %>% mutate_if(is.numeric, round, digits = n_digits)
}

probability_of_superiority_function <- function(x, y) {
  nx <- length(x)
  ny <- length(y)
  rx <- sum(rank(c(x, y))[1:nx])
  A = (rx / nx - (nx + 1) / 2) / ny
  return(A)
}

spearman_brown_correction <- function(r_pearson) {
  inversion <- ifelse(r_pearson < 0, -1, 1)
  val <- round(2*abs(r_pearson)/(1 + abs(r_pearson)), 2)
  return(val*inversion)
}

apa_p_value <- function(p){
  p_formatted <- ifelse(p >= 0.001, paste("=", round(p, 3)),
                        ifelse(p < 0.001, "< .001", NA))
  p_formatted <- gsub(pattern = "0.", replacement = ".", x = p_formatted, fixed = TRUE)
  p_formatted
}

add_heterogeneity_metrics_to_forest <- function(fit) {
  ## more detailed
  # bquote(paste("RE Model (Q(df = ", .(formatC(fit$k - 1)), ") = ", 
  #              .(formatC(round(fit$QE, 2))), 
  #              ", p ", .(formatC(apa_p_value(fit$QEp))),
  #              ", ", tau^2, " = ", .(formatC(round(fit$tau2, 1))), 
  #              ", ", I^2, " = ", .(formatC(round(fit$I2, 1))),
  #              "%, ", H^2," = ", .(formatC(round(fit$H2, 1))), ")"))
  ## less detailed
  bquote(paste("RE Model (", tau^2, " = ", .(formatC(round(fit$tau2, 1))), 
               ", ", I^2, " = ", .(formatC(round(fit$I2, 1))),
               "%, ", H^2," = ", .(formatC(round(fit$H2, 1))), ")"))
}

# table format in output
options(knitr.table.format = "html") 

```

# Data 

and exclusions for incorrect number of trials

```{r}

# INTERNAL CONSISTENCY

# Two different data sources (datasets that were had their initial cleaning and organizing done at different times): do final cleaning on these and add them together.

# get data from evaluative IRAPs
data_input_evaluative <- 
  read.csv("../data/evaluative IRAPs/processed/combined_latency_data.csv") %>%
  mutate(trial_odd_even = ifelse(row_number() %% 2 == 1, "D_odd", "D_even")) %>%
  # tidy domain
  rename(domain = experiment) %>%
  mutate(domain = dplyr::recode(domain,
                                `chad bodyimage` = "Body shape evaluations",
                                `chad friendenemy` = "Friend-Enemy evaluations",
                                `chad gender` = "Gender stereotypes",
                                `chad lincolnhitler` = "Lincoln-Hitler evaluations",
                                `chad race`	 = "Racial evaluations 2",
                                `chad religion` = "Christianity-Islam",
                                `ian death emma` = "Life/Death evaluations 1",
                                `ian death june` = "Life/Death evaluations 2",
                                `ian death tarah` = "Life/Death evaluations 3",
                                `ian race dearbhail` = "Racial evaluations 1",
                                `ian race katie` = "Racial evaluations 1")) %>%
  arrange(as.character(domain))

# do exclusions for unique ids with wrong number of trials
data_participants_with_correct_n_trials <- data_input_evaluative %>%
  count(unique_id) %>%
  filter(n %in% c(144, 192)) # methodologically plausible number of trials

data_cleaned_ic_evaluative <- semi_join(data_input_evaluative, data_participants_with_correct_n_trials, by = "unique_id")



# get data from nonevaluative IRAPs
# nb exclusions for unique ids with wrong number of trials already done in processing
data_cleaned_ic_nonevaluative <- 
  read.csv("../data/nonevaluative IRAPs/processed/combined_latency_data.csv") %>%
  mutate(trial_odd_even = ifelse(row_number() %% 2 == 1, "D_odd", "D_even")) %>%
  # tidy domain
  rename(domain = experiment) %>%
  mutate(domain = dplyr::recode(domain,
                                `countries acknowledge 1` = "Countries - acknowledge suffering 1",
                                `countries care 1` = "Countries - care about suffering 1",
                                `countries 2 acknowledge` = "Countries - acknowledge suffering 2",
                                `countries 2 care` = "Countries - care about suffering 2",
                                `disgust` = "Digust sensitivity",
                                `positive_negative` = "Valenced categories evaluations",
                                `sexuality` = "Sexuality and arousal",
                                `food hunger` = "Food and hunger",
                                `shapes and colors YN rule` = "Shapes and colors 1",
                                `shapes and colors YN correct` = "Shapes and colors 2",
                                `shapes and colors TF correct` = "Shapes and colors 3",
                                `shapes and colors TF rule` = "Shapes and colors 4",
                                `shapes and colors exp 2 full rule` = "Shapes and colors 5",
                                `shapes and colors exp 2 minimal rule` = "Shapes and colors 6",
                                `shapes and colors exp 3` = "Shapes and colors 7")) %>%
  arrange(as.character(domain)) 

# combined
data_combined_ic <- bind_rows(data_cleaned_ic_evaluative, 
                           data_cleaned_ic_nonevaluative) %>%
  mutate(domain = as.factor(as.character(domain)))

# remove participant with the wrong number of trials/duplicate data
data_n_trials <- data_combined_ic %>%
  group_by(unique_id) %>%
  summarise(n = n()) %>%
  filter(n %in% c(144, 192, 240))

data_cleaned_ic <- data_combined_ic %>%
  semi_join(data_n_trials, by = "unique_id")



# TEST RETEST

data_cleaned_trt <- 
  read.csv("../data/test-retest IRAPs/processed/combined_latency_data.csv") %>%
  # tidy domain
  rename(domain = experiment) %>%
  mutate(domain = dplyr::recode(domain,
                                `disgust` = "Digust sensitivity (immediate followup)",
                                `sexuality test retest` = "Sexuality and arousal (7 day followup)")) %>%
  arrange(as.character(domain))

```

# Demographics & sample sizes

```{r}

data_demographics <- 
  bind_rows(mutate(data_cleaned_ic, type = "internal consistency"),
          mutate(data_cleaned_trt, type = "test retest")) %>%
  mutate(unique_id_and_type = paste(unique_id, type)) %>%
  distinct(unique_id, .keep_all = TRUE)

data_demographics %>%
  summarize(mean_age = mean(age, na.rm = TRUE),
            sd_age = sd(age, na.rm = TRUE)) %>% 
  round_df(1) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_demographics %>%
  count(gender) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_demographics %>%
  group_by(type) %>%
  summarize(n = n()) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

# Test-retest reliability

## Calculate *D* scores

```{r}

data_D_scores <- data_cleaned_trt %>%
  filter(rt <= 10000) %>%
  group_by(unique_id, timepoint) %>%
  summarize(mean_con = mean(rt[block_type == "con"], na.rm = TRUE),
            mean_incon = mean(rt[block_type == "incon"], na.rm = TRUE),
            sd = mean(rt, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(D = (mean_incon - mean_con)/sd) %>%
  select(unique_id, D, timepoint) %>%
  spread(timepoint, D) %>%
  left_join(select(distinct(data_cleaned_trt, unique_id, .keep_all = TRUE), unique_id, domain), by = "unique_id")

```

## Test-retest absolute agreement 

I..e, Intraclass Correlation Coefficients.

"It has been argued that test-retest reliability should reflect absolute agreement, rather than consistency, between measurements (Koo & Li, 2016). For example, a perfect correlation between scores at two time points may occur also when there is a systematic difference between time points (i.e. a difference that is about equal for all participants)." (Parsons, Kruijt, & Fox, 2019).
Absolute agreement therefore also takes within-participant changes into account in its denominator, where consistency does not.

```{r}

fit_digust <- data_D_scores %>%
  filter(domain == "Digust sensitivity (immediate followup)") %>%
  select(baseline, followup) %>%
  ICC(lmer = TRUE)

icc_disgust <- fit_digust$results %>%
  filter(type == "ICC2") %>%
  mutate(domain = "Digust sensitivity (immediate followup)") %>%
  select(domain, type, ICC, "lower bound", "upper bound") %>%
  mutate(se = (`upper bound` - `lower bound`)/(1.96*2))


fit_sexuality <- data_D_scores %>%
  filter(domain == "Sexuality and arousal (7 day followup)") %>%
  select(baseline, followup) %>%
  ICC(lmer = TRUE)

icc_sexuality <- fit_sexuality$results %>%
  filter(type == "ICC2") %>%
  mutate(domain = "Sexuality and arousal (7 day followup)") %>%
  select(domain, type, ICC, "lower bound", "upper bound") %>%
  mutate(se = (`upper bound` - `lower bound`)/(1.96*2))


data_icc_combined <- 
  bind_rows(icc_disgust,
            icc_sexuality) %>%
  round_df(2) 

```

Variance associated with each effect size calculated mathematically using the effect size and sample size.

```{r fig.height=3.2, fig.width=8}

# fit random Effects model 
fit_aa <- data_icc_combined %>%
  filter(type == "ICC2") %>%
  rma(yi   = ICC, 
      sei  = se, 
      data = .,
      slab = domain)

# make predictions 
predictions <-
  predict(fit_aa, digits = 5) %>%
  as.data.frame() %>%
  round_df(2) 

# plot
metafor::forest(fit_aa,
                xlab = "Absolute agreement ICC",
                addcred = TRUE,
                refline = FALSE,
                xlim = c(-1.5, 1.7),
                at = c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
                mlab = add_heterogeneity_metrics_to_forest(fit_aa))
text(-1.5, 4, "Study", pos = 4)
text(1.7, 4, "ICC [95% CI]", pos = 2)

# summarize results
meta_effect <- 
  paste0("Meta analysis: k = ", fit_aa$k, 
         ", ICC2 = ",  round(transf.ztor(predictions$pred), 2), 
         ", 95% CI [", round(transf.ztor(predictions$ci.lb), 2), ", ", 
         round(transf.ztor(predictions$ci.ub), 2), "]", 
         ", 95% CR [", round(transf.ztor(predictions$cr.lb), 2), ", ", 
         round(transf.ztor(predictions$cr.ub), 2), "]") 

meta_heterogeneity <- 
  paste0("Heterogeneity tests: Q(df = ", fit_aa$k - 1, ") = ", round(fit_aa$QE, 2), 
         ", p ", ifelse(fit_aa$QEp < 0.0001, "< .0001", paste0("= ", as.character(round(fit_aa$QEp, 4)))),
         ", tau^2 = ", round(fit_aa$tau2, 2), 
         ", I^2 = ",   round(fit_aa$I2, 2),
         "%, H^2 = ",   round(fit_aa$H2, 2))

# write to disk for pdf plots
write_rds(fit_aa, "models/fit_aa.rds")

```

Meta effect: `r meta_effect`.

Heterogeneity: `r meta_heterogeneity`.

## Implications of task length

### How to improve via lengthening

The [Spearman-Brown prediction formula](https://en.wikipedia.org/wiki/Spearman%E2%80%93Brown_prediction_formula#Forecasting_test_length) can be rearranged to make prediction about how the length of the test would need to change to meet a goal reliability:

${\rho }_{xx'}^{*}={\frac {n{\rho }_{xx'}}{1+(n-1){\rho }_{xx'}}}$

where ${\rho }_{xx'}^{*}$ refers to the goal reliability, ${\rho }_{xx'}$ refers to the current reliability, and ${n}$ refers to the test length multiplier.

```{r}

current_test_retest <- transf.ztor(predictions$pred)

goal_test_retest <- 0.90
length_increase_.90 <- round((goal_test_retest*(1 - current_test_retest)) / (current_test_retest*(1 - goal_test_retest)), 1)

goal_test_retest <- 0.80
length_increase_.80 <- round((goal_test_retest*(1 - current_test_retest)) / (current_test_retest*(1 - goal_test_retest)), 1)

goal_test_retest <- 0.70
length_increase_.70 <- round((goal_test_retest*(1 - current_test_retest)) / (current_test_retest*(1 - goal_test_retest)), 1)

```

Using the absolute agreement estimate of 0.40, the IRAP would have to be `r length_increase_.70` times longer for it to have a test retest of >=.70, `r length_increase_.80` times longer for it to have a test retest of >=.80, or `r length_increase_.90` times longer for it to have a test retest of >=.90.

# Internal consistency

Three methods of calculating internal consistency for the Implicit Relational Assessment Procedure (IRAP): 
- Odd vs even trials (mostly commonly reported in literature)
- First vs second half of task (more common for the most popular implicit measure, the IAT; would allow a like-for-like comparison)
- Cronbach's alpha via permutation (most robust)

The IRAP effect is most often quantified using the D1 scoring algorithm (Greenwald, Banaji & Nosek, 2003). i.e., trim all RTs > 10,000 ms, then D1 = (mean RT in the inconsistent blocks - mean RT in the consistent blocks)/SD of RTs in both block types. D1 scores are used throughout. Because of this, it wasn't possible to use Sam Parson's splithalf R package. A workflow for permutation-based alpha estimation using D scores is provided below.  

In each case, correlations between D scores calculated from the split halves are calculated, and then the Spearman-Brown prophecy formula is applied, i.e., r_sb = 2*r_pearson / (1+r_pearson). Importantly, this correction is applied to the absolute value of the pearson's correlation, and then its sign is corrected to be congruent with the pearson's correlation. This ensures that r_sb has a possible range of -1 to +1, as correlations should (whereas correction of native negative correlations allow for a lower limit of -Inf). I haven't seen this treatment of SB corrections applied to negative correlations well explicated elsewhere, so it is important to note here. 

For meta analysis, spearman brown correlations are converted to Fischers r-to-z for meta analysis. Estimates of the meta effect are then converted back for reporting. Heterogeneity metrics represent heterogeneity in Fischer's r-to-z estimates. See [here](https://stats.stackexchange.com/questions/26485/does-transformation-of-r-into-fisher-z-benefit-a-meta-analysis).

## Split-half reliability via odd vs. even trials

### Calculate D scores

```{r}

data_D_scores <- data_cleaned_ic %>%
  filter(rt <= 10000) %>%
  group_by(unique_id, trial_odd_even) %>%
  summarize(mean_con = mean(rt[block_type == "con"], na.rm = TRUE),
            mean_incon = mean(rt[block_type == "incon"], na.rm = TRUE),
            sd = mean(rt, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(D = (mean_incon - mean_con)/sd) %>%
  select(unique_id, D, trial_odd_even) %>%
  spread(trial_odd_even, D) %>%
  left_join(select(distinct(data_cleaned_ic, unique_id, .keep_all = TRUE), unique_id, domain), by = "unique_id")

```

### Plot

```{r fig.height=8, fig.width=8}

ggplot(data_D_scores, aes(D_odd, D_even)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm") +
  facet_wrap(~domain)

```

### Meta analysis

Bonett transformations 

```{r fig.height=8, fig.width=8}

# calculate internal consistencies
data_combined <- data_D_scores %>%
  dplyr::select(domain, D_odd, D_even) %>%
  na.omit() %>%
  group_by(domain) %>%
  summarize(r_pearson = cor(D_odd, D_even),
            n = n()) %>%
  ungroup() %>%
  mutate(r_sb = spearman_brown_correction(r_pearson),
         parts = 2) %>%
  escalc(measure = "ABT", 
         ai = r_sb, 
         mi = parts, 
         ni = n,
         data = .) 

# fit random Effects model 
fit_oddeven <- 
  rma(yi   = yi, 
      vi   = vi, 
      data = data_combined,
      slab = domain)

# make predictions 
predictions <-
  predict(fit_oddeven, digits = 5) %>%
  as.data.frame() %>%
  round_df(2)

# plot
metafor::forest(fit_oddeven,
                xlab = "Spearman-Brown correlation",
                addcred = TRUE,
                refline = FALSE,
                transf = transf.iabt,
                xlim = c(-1.4, 1.6),
                at = c(0, 0.25, 0.5, 0.75, 1),
                mlab = add_heterogeneity_metrics_to_forest(fit_oddeven))
text(-1.4, 27, "Study", pos = 4)
text(1.6, 27, "Spearman-Brown correlation [95% CI]", pos = 2)

# summarize results
meta_effect <- 
  paste0("Meta analysis: k = ", fit_oddeven$k, 
         ", r_sb = ",  round(transf.ztor(predictions$pred), 2), 
         ", 95% CI [", round(transf.ztor(predictions$ci.lb), 2), ", ", 
         round(transf.ztor(predictions$ci.ub), 2), "]", 
         ", 95% CR [", round(transf.ztor(predictions$cr.lb), 2), ", ", 
         round(transf.ztor(predictions$cr.ub), 2), "]") 

meta_heterogeneity <- 
  paste0("Heterogeneity tests: Q(df = ", fit_oddeven$k - 1, ") = ", round(fit_oddeven$QE, 2), 
         ", p ", ifelse(fit_oddeven$QEp < 0.0001, "< .0001", paste0("= ", as.character(round(fit_oddeven$QEp, 4)))),
         ", tau^2 = ", round(fit_oddeven$tau2, 2), 
         ", I^2 = ",   round(fit_oddeven$I2, 2),
         "%, H^2 = ",   round(fit_oddeven$H2, 2))

```

Meta effect: `r meta_effect`.

Heterogeneity: `r meta_heterogeneity`.

## Split-half reliability via first vs. second half of the task

### Calculate D scores

```{r}

data_n_trials <- 
  data_cleaned_ic %>%
  group_by(unique_id) %>%
  summarize(total_n_trials = n()) 

data_halves <- data_cleaned_ic %>%
  left_join(data_n_trials, by = "unique_id") %>%
  group_by(unique_id) %>%
  mutate(trial_n = row_number(),
         half = ifelse(trial_n > (total_n_trials/2), "second", "first")) %>%
  ungroup()

data_D_scores_firstsecond <- data_halves %>%
  filter(rt <= 10000) %>%
  group_by(unique_id, half) %>%
  summarize(mean_con = mean(rt[block_type == "con"], na.rm = TRUE),
            mean_incon = mean(rt[block_type == "incon"], na.rm = TRUE),
            sd = mean(rt, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(D = (mean_incon - mean_con)/sd) %>%
  select(unique_id, D, half) %>%
  spread(half, D) %>%
  rename(D_first = first, 
         D_second = second) %>%
  left_join(select(distinct(data_cleaned_ic, unique_id, .keep_all = TRUE), unique_id, domain), by = "unique_id")

```

### Plot

```{r fig.height=8, fig.width=8}

ggplot(data_D_scores_firstsecond, aes(D_first, D_second)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm") +
  facet_wrap(~domain)

```

### Meta analysis

Bonett transformations 

```{r fig.height=8, fig.width=8}

# calculate internal consistencies
data_combined_firstsecond <- data_D_scores_firstsecond %>%
  dplyr::select(domain, D_first, D_second) %>%
  na.omit() %>%
  group_by(domain) %>%
  summarize(r_pearson = cor(D_first, D_second),
            n = n()) %>%
  ungroup() %>%
  mutate(r_sb = spearman_brown_correction(r_pearson),
         parts = 2) %>%
  escalc(measure = "ABT", 
         ai = r_sb, 
         mi = parts, 
         ni = n,
         data = .) 

# fit random Effects model 
fit_firstsecond <- 
  rma(yi   = yi, 
      vi   = vi, 
      data = data_combined_firstsecond,
      slab = domain)

# make predictions 
predictions <-
  predict(fit_firstsecond, digits = 5) %>%
  as.data.frame() %>%
  round_df(2)

# plot
metafor::forest(fit_firstsecond,
                xlab = "Spearman-Brown correlation",
                addcred = TRUE,
                refline = FALSE,
                transf = transf.iabt,
                xlim = c(-1.4, 1.6),
                at = c(0, 0.25, 0.5, 0.75, 1),
                mlab = add_heterogeneity_metrics_to_forest(fit_firstsecond))
text(-1.4, 27, "Study", pos = 4)
text(1.6, 27, "Spearman-Brown correlation [95% CI]", pos = 2)

# summarize results
meta_effect <- 
  paste0("Meta analysis: k = ", fit_firstsecond$k, 
         ", r_sb = ",  round(transf.ztor(predictions$pred), 2), 
         ", 95% CI [", round(transf.ztor(predictions$ci.lb), 2), ", ", 
         round(transf.ztor(predictions$ci.ub), 2), "]", 
         ", 95% CR [", round(transf.ztor(predictions$cr.lb), 2), ", ", 
         round(transf.ztor(predictions$cr.ub), 2), "]") 

meta_heterogeneity <- 
  paste0("Heterogeneity tests: Q(df = ", fit_firstsecond$k - 1, ") = ", round(fit_firstsecond$QE, 2), 
         ", p ", ifelse(fit_firstsecond$QEp < 0.0001, "< .0001", paste0("= ", as.character(round(fit_firstsecond$QEp, 4)))),
         ", tau^2 = ", round(fit_firstsecond$tau2, 2), 
         ", I^2 = ",   round(fit_firstsecond$I2, 2),
         "%, H^2 = ",   round(fit_firstsecond$H2, 2))

```

Meta effect: `r meta_effect`.

Heterogeneity: `r meta_heterogeneity`.


## Permutation-based split half correlations 

ie an estimate of alpha through large number number of random half splits (see Parsons, Kruijt, & Fox. 2019).

### Calculate D scores

Because of the large number of D scores that are calculated (i.e., 2 per participant \* 437 participants \* 2000 permutations = 1.75 million D scores) this section can take a few minutes to run. I've already done some timing optimization to speed up this process by a factor of 5, but perhaps more is possible.

```{r}

# slow runtime - circa 10 minutes, so check if there's saved results already first
if (file.exists("models/data_internal_consistency_permutations.RData")) {
  
  load("models/data_internal_consistency_permutations.RData")
  
} else {
  
  # n iterations
  n_iterations <- 5000
  
  # find domain for each id - to be joined into df further below
  data_domain_for_each_id <- data_cleaned_ic %>%
    distinct(unique_id, .keep_all = TRUE) %>%
    select(unique_id, domain)
  
  # create a temp list
  data_internal_consistency_list = list()
  
  # define progress bar
  step_i <- 0
  pb = txtProgressBar(min = 0, max = n_iterations, initial = 0)
  
  # assign row ids
  data_cleaned_ic_rownumbers <- data_cleaned_ic %>%
    arrange(unique_id, block_type) %>%
    rownames_to_column(var = "row_id") %>%
    group_by(unique_id, block_type) %>%
    mutate(trial_number = row_number()) %>%
    ungroup()
  
  # apply workflow and append results to list
  for(i in 1:n_iterations){
    
    # sample exactly half of the data and label as subset 'a' vs 'b'
    max_trials_per_block <- data_cleaned_ic_rownumbers %>%
      summarize(max_trials_per_block = max(trial_number)) %>%
      pull(max_trials_per_block)
    
    trial_subset_a <- data.frame(trial_number = seq(from = 1, to = max_trials_per_block, by = 1)) %>% 
      sample_frac(size = 0.5) %>%
      arrange(trial_number) %>%
      pull(trial_number)
    
    data_cleaned_ic_rownumbers_with_subsets <- data_cleaned_ic_rownumbers %>%
      mutate(subset = ifelse(trial_number %in% trial_subset_a, "a", "b")) %>%
      arrange(unique_id, block_type, trial_number)
    
    # calculate a metric of how ordered this randomized subset of the trials are
    ## start by creating the 'b' vector
    temp <- seq(from = 1, to = max_trials_per_block, by = 1)
    trial_subset_b <- temp[!temp %in% trial_subset_a]
    
    ## probability that b > a
    probability_of_superiority <- probability_of_superiority_function(trial_subset_b, trial_subset_a)
    
    subset_orderedness <- abs(probability_of_superiority - 0.50) + 0.5
    
    # then calculate D1 scores (Greenwald, Banaji & Nosek, 2003)
    # these involve: 1) trimming RTs > 10000 ms, 2) finding a mean rt for each block type, 3) findings an sd for all the pooled RTs across blocks, 4) D = (mean_incon - mean_con)/sd
    data_temp_1 <- data_cleaned_ic_rownumbers_with_subsets %>%
      filter(rt <= 10000)
    
    data_temp_2 <- data_temp_1 %>%
      group_by(unique_id, subset, block_type) %>%
      summarize(mean = mean(rt, na.rm = TRUE)) %>%
      ungroup() %>%
      spread(block_type, mean) %>%
      rename(mean_con = con,
             mean_incon = incon)
    
    data_temp_3 <- data_temp_1 %>%
      group_by(unique_id, subset) %>%
      summarize(sd = mean(rt, na.rm = TRUE)) %>%
      ungroup()
    
    data_internal_consistency <- left_join(data_temp_2, data_temp_3, by = c("unique_id", "subset")) %>%
      # finish calculating D scores for each participant and subset
      mutate(D = (mean_incon - mean_con)/sd) %>%
      select(unique_id, D, subset) %>%
      spread(subset, D) %>%
      rename(D_a = a,
             D_b = b) %>%
      # re add the study domain
      left_join(data_domain_for_each_id, by = "unique_id") %>%
      # calculate correlations between subsets
      na.omit() %>%
      group_by(domain) %>%
      summarize(r_pearson = cor(D_a, D_b)) %>%
      ungroup() %>%
      # apply spearman brown correction
      mutate(r_sb = spearman_brown_correction(r_pearson)) %>%
      # add iteration
      mutate(iteration = i,
             orderedness = subset_orderedness)
    
    data_internal_consistency_list[[i]] <- data_internal_consistency
    
    # update progress bar
    step_i = step_i + 1
    setTxtProgressBar(pb, step_i)
    
  }
  
  # flatten list and parse results
  data_internal_consistency_permutations <- dplyr::bind_rows(data_internal_consistency_list)
  
  # save
  save(data_internal_consistency_permutations, 
       file = "models/data_internal_consistency_permutations.RData")
  
}

```

### Plot

```{r fig.height=5, fig.width=5}

ggplot(data_internal_consistency_permutations, aes(r_sb)) +
  geom_vline(aes(xintercept = 0), linetype = "dashed") +
  geom_density() +
  facet_wrap(~domain) +
  coord_cartesian(xlim = c(-0.5, 1.0))

# # I explored calculating a metric of 'orderedness' of the permutations (i.e., how close they were to the actual order of trials) in order to attempt to assess whether there was evidence that there is learning within the task. However, random sampling greatly undersamples highly ordered permutations, which are statistically rare. I've left this code here in case this issue is returned to later.  
# ggplot(data_internal_consistency_permutations, aes(orderedness, r_sb)) +
#   geom_point(alpha = 0.1) +
#   geom_smooth(method = "lm") +
#   xlab("permutation orderedness \n(absolute probability of subset a trial number > subset b trial number)") +
#   ylab("spearman-brown corrected correlation")

data_internal_consistency_estimates <- data_internal_consistency_permutations %>%
  group_by(domain) %>%
  dplyr::summarize(yi       = mean(r_sb, na.rm = TRUE),
                   ci_lower = quantile(r_sb, 0.025, na.rm = TRUE),
                   ci_upper = quantile(r_sb, 0.975, na.rm = TRUE))

# data_internal_consistency_estimates %>%
#   round_df(2) %>%
#   arrange(yi) %>%
#   kable() %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
# 
# ggplot(data_internal_consistency_estimates, aes(domain, yi)) +
#   geom_point() +
#   geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
#   coord_flip() +
#   geom_hline(yintercept = 0, linetype = "dashed")

```

### Meta analysis

Estimate of alpha obtained via mean of the resampled Spearman-Brown corrected split half correlations, when calculating D1 scores for each half and sampling 5000 permutations.   

Bonett transformations 

NB forest plot above includes empirical CIs across the permutation resamples using the percentile method, whereas the forest plot below estimates CIs based on the variance calculated from the Fischer's r-to-z and N. 

```{r fig.height=8, fig.width=8}

data_n_per_domain <- data_cleaned_ic %>%
  distinct(unique_id, .keep_all = TRUE) %>%
  count(domain) %>%
  select(domain, n)

data_internal_consistency_estimates_sb <- data_internal_consistency_permutations %>%
  group_by(domain) %>%
  dplyr::summarize(r_sb = mean(r_sb, na.rm = TRUE)) %>%
  left_join(data_n_per_domain, by = "domain") %>%
  mutate(parts = 2) %>%
  escalc(measure = "ABT", 
         ai = r_sb, 
         mi = parts, 
         ni = n,
         data = .)

# fit random Effects model 
fit_permutations <- 
  rma(yi   = yi, 
      vi   = vi, 
      data = data_internal_consistency_estimates_sb,
      slab = domain)

# make predictions 
predictions <-
  predict(fit_permutations, digits = 5) %>%
  as.data.frame() %>%
  round_df(2)

# plot
metafor::forest(fit_permutations,
                xlab = bquote(paste("Cronbach's ", alpha)),
                addcred = TRUE,
                refline = FALSE,
                transf = transf.iabt,
                xlim = c(-1.4, 1.6),
                at = c(0, 0.25, 0.5, 0.75, 1),
                mlab = add_heterogeneity_metrics_to_forest(fit_permutations))
text(-1.4, 27, "Study", pos = 4)
text(1.6, 27, bquote(paste("Cronbach's ", alpha, " [95% CI]")), pos = 2)

# summarize results
meta_effect <- 
  paste0("Meta analysis: k = ", fit_permutations$k, 
         ", alpha = ",  round(transf.ztor(predictions$pred), 2), 
         ", 95% CI [", round(transf.ztor(predictions$ci.lb), 2), ", ", 
         round(transf.ztor(predictions$ci.ub), 2), "]", 
         ", 95% CR [", round(transf.ztor(predictions$cr.lb), 2), ", ", 
         round(transf.ztor(predictions$cr.ub), 2), "]") 

meta_heterogeneity <- 
  paste0("Heterogeneity tests: Q(df = ", fit_permutations$k - 1, ") = ", round(fit_permutations$QE, 2), 
         ", p ", ifelse(fit_permutations$QEp < 0.0001, "< .0001", paste0("= ", as.character(round(fit_permutations$QEp, 4)))),
         ", tau^2 = ", round(fit_permutations$tau2, 2), 
         ", I^2 = ",   round(fit_permutations$I2, 2),
         "%, H^2 = ",   round(fit_permutations$H2, 2))

# save to disk for making pdf plot
write_rds(fit_permutations, "models/fit_permutations.rds")

```

Meta effect: `r meta_effect`.

Heterogeneity: `r meta_heterogeneity`.

## Implications of task length

### Power implications

r_observed = r_true \* sqrt(ic_a*ic_b)

```{r}

observed_r <- function(true_r, reliability_a, reliability_b) {
  round(true_r * sqrt(reliability_a*reliability_b), 2)
}

max_cor <- observed_r(true_r = 1.0, reliability_a = transf.iabt(predictions$pred), reliability_b = 1.0)

```

Given this meta estimate of internal consistency, assuming that the internal consistency of an external variable is perfect (1.0), the maximum correlation between the IRAP and that external variable is `r max_cor`. This has implications for power and sample size determination, which would therefore need to be roughly 33% larger than typically estimated if the IRAP's reliability was taken into account (see Parsons REF).

### How to improve via lengthening

The [Spearman-Brown prediction formula](https://en.wikipedia.org/wiki/Spearman%E2%80%93Brown_prediction_formula#Forecasting_test_length) can be rearranged to make prediction about how the length of the test would need to change to meet a goal reliability:

${\rho }_{xx'}^{*}={\frac {n{\rho }_{xx'}}{1+(n-1){\rho }_{xx'}}}$

where ${\rho }_{xx'}^{*}$ refers to the goal reliability, ${\rho }_{xx'}$ refers to the current reliability, and ${n}$ refers to the test length multiplier.

```{r}

current_ic <- transf.iabt(predictions$pred)

goal_ic <- 0.70
length_increase_.70 <- round((goal_ic*(1 - current_ic)) / (current_ic*(1 - goal_ic)), 1)

goal_ic <- 0.80
length_increase_.80 <- round((goal_ic*(1 - current_ic)) / (current_ic*(1 - goal_ic)), 1)

goal_ic <- 0.90
length_increase_.90 <- round((goal_ic*(1 - current_ic)) / (current_ic*(1 - goal_ic)), 1)

```

Using $a$ = `r round(transf.iabt(predictions$pred), 2)`, the IRAP would have to be `r length_increase_.70` times longer for it to have an internal consistency of >=.70, `r length_increase_.80` times longer for it to have an internal consistency of >=.80, or `r length_increase_.90` times longer for it to have an internal consistency of >=.90.



